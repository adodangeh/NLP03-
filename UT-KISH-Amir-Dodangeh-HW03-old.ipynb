{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UT-KISH-Amir-Dodangeh-HW03-old.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPBpirJrvaapN2YPCPO5UWy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"bhbLXOOo-B1j","executionInfo":{"status":"ok","timestamp":1623171685639,"user_tz":-270,"elapsed":2307,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}},"outputId":"0610dd4f-8bfe-495b-bc28-870abbbc38c0"},"source":["\n","\n","import pandas as pd\n","import pandas as pd\n","import csv\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","#with open(\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica 3.xslx\", 'rb') as f:\n","#  df = f.read()\n","\n","df= pd.read_excel(open(\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica 3.xlsx\", 'rb')) \n","df.head(10)\n","\n","#from sklearn.feature_extraction.text import TfidfVectorizer\n","#tfidf = TfidfVectorizer()\n","#tfidf.fit(df[])\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Row</th>\n","      <th>NewsID</th>\n","      <th>Title</th>\n","      <th>Text</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>Category two</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>843656</td>\n","      <td>وزير علوم درجمع استادان نمونه: سن بازنشستگي اس...</td>\n","      <td>وزير علوم در جمع استادان نمونه كشور گفت: از اس...</td>\n","      <td>138/5//09</td>\n","      <td>0:9::18</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>837144</td>\n","      <td>گردهمايي دانش‌آموختگان موسسه آموزش عالي سوره ب...</td>\n","      <td>به گزارش سرويس صنفي آموزشي خبرگزاري دانشجويان ...</td>\n","      <td>138/5//09</td>\n","      <td>1:4::11</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>436862</td>\n","      <td>نتايج آزمون دوره‌هاي فراگير دانشگاه پيام‌نور ا...</td>\n","      <td>نتايج آزمون دوره‌هاي فراگير مقاطع كارشناسي و ك...</td>\n","      <td>138/3//07</td>\n","      <td>1:0::03</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>174187</td>\n","      <td>وضعيت اقتصادي و ميزان تحصيلات والدين از مهمتري...</td>\n","      <td>محمدتقي علوي يزدي، مجري اين طرح پژوهشي در اين‌...</td>\n","      <td>138/1//08</td>\n","      <td>1:1::49</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>768531</td>\n","      <td>دانشكده علوم حديث همزمان با سال پيامبر اعظم (ص...</td>\n","      <td>دوره آموزشي سيره نبوي بعنوان يكي از رشته‌هاي ج...</td>\n","      <td>138/5//05</td>\n","      <td>1:1::18</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>395642</td>\n","      <td>نخستين مركز آموزش عالي بين‌المللي دانشگاه مونا...</td>\n","      <td>به گزارش خبرنگار اعزامي خبرگزاري دانشجويان اير...</td>\n","      <td>138/3//03</td>\n","      <td>1:1::52</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>176676</td>\n","      <td>دومين گردهمايي انجمن‌هاي علمي دانشجويي دانشگاه...</td>\n","      <td>به گزارش سرويس آموزشي ايسنا، دفتر انجمن‌هاي عل...</td>\n","      <td>138/1//09</td>\n","      <td>0:9::44</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>787861</td>\n","      <td>تقويم تحصيلي دانشگاه پيام نور مشهد اعلام شد</td>\n","      <td>به گزارش خبرگزاري دانشجويان ايران (ايسنا) - من...</td>\n","      <td>138/5//06</td>\n","      <td>0:8::38</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>142104</td>\n","      <td>هفتمين اجلاس مشترك هياتهاي امناي دانشگاهها و م...</td>\n","      <td>به گزارش خبرنگار آموزشي ايسنا، در اين اجلاس يك...</td>\n","      <td>138/1//04</td>\n","      <td>1:1::58</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>154915</td>\n","      <td>وزارت علوم: خبر فروش رشته‌هاي دانشگاهي روزنامه...</td>\n","      <td>به گزارش سرويس آموزشي ايسنا، در اين نمابر آمده...</td>\n","      <td>138/1//06</td>\n","      <td>1:2::08</td>\n","      <td>آموزشي-</td>\n","      <td>آموزشي</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Row  NewsID  ... Category two Category\n","0    1  843656  ...      آموزشي-   آموزشي\n","1    2  837144  ...      آموزشي-   آموزشي\n","2    3  436862  ...      آموزشي-   آموزشي\n","3    4  174187  ...      آموزشي-   آموزشي\n","4    5  768531  ...      آموزشي-   آموزشي\n","5    6  395642  ...      آموزشي-   آموزشي\n","6    7  176676  ...      آموزشي-   آموزشي\n","7    8  787861  ...      آموزشي-   آموزشي\n","8    9  142104  ...      آموزشي-   آموزشي\n","9   10  154915  ...      آموزشي-   آموزشي\n","\n","[10 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ENM6zlz0-CLg"},"source":["#NEW"]},{"cell_type":"code","metadata":{"id":"rz6A8sKu4b00","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623171180334,"user_tz":-270,"elapsed":32727,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}},"outputId":"3ed2ffa0-4fb6-44f4-a760-595ba73da1ed"},"source":["import pandas as pd\n","import csv\n","import sys\n","from google.colab import drive\n","\n","\n","drive.mount('/content/gdrive')\n","\n","\n","#with open(\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\", 'rb') as f:\n","#  data1 = f.read()\n","\n","#corpus =pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\", error_bad_lines=False)\n","\n","\n","\n","#corona_df = pd.read_csv('data.csv')\n","#corona_df.dropna(subset=['text_body'],inplace=True)\n","#corona_df.head()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnfjcER7k52L","executionInfo":{"status":"ok","timestamp":1623171776690,"user_tz":-270,"elapsed":4809,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}},"outputId":"e7b80c9b-75bb-42c0-92f0-7705d3d78aff"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf = TfidfVectorizer()\n","tfidf.fit(df['Text'])"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.float64'>, encoding='utf-8',\n","                input='content', lowercase=True, max_df=1.0, max_features=None,\n","                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n","                smooth_idf=True, stop_words=None, strip_accents=None,\n","                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, use_idf=True, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"Hi8dMDpAlGiR","executionInfo":{"status":"ok","timestamp":1623171829530,"user_tz":-270,"elapsed":5642,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}},"outputId":"31335465-10bb-4bad-b8db-02d3aa32d2e9"},"source":["X = tfidf.transform(df['Text'])\n","df['Text'][1]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'به گزارش سرويس صنفي آموزشي خبرگزاري دانشجويان ايران (ايسنا)، در اين برنامه كه به منظور تاسيس و انتخابات جامعه دانش\\u200cآموختگان سوره برگزار مي\\u200cشود جمعي از مسئولان و مديران سازمان تبليغات اسلامي، مديران حوزه هنري، مديران موسسه آموزش عالي سوره و همچنين جمعي از هنرمندان، استادان و اعضاي هيات علمي اين موسسه حضور دارند. همچنين در اين برنامه از جمعي از دانش آموختگان نمونه و استادان پيشكسوت اين موسسه تقدير مي\\u200cشود. اجراي برنامه\\u200cهاي موسيقي، تئاتر و نمايشگاهي از فعاليت\\u200cهاي دانشجويان در زمينه\\u200cهاي فرهنگي و هنري در گذشته و حال و همچنين غرفه\\u200cهاي عكس يادگاري و ذكر خاطره\\u200cاي از درگذشتگان سوره از جمله برنامه\\u200cهاي جنبي اين گردهمايي مي\\u200cباشد.'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"id":"hxB1DTeUephU","executionInfo":{"status":"error","timestamp":1623171182680,"user_tz":-270,"elapsed":2353,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}},"outputId":"d32efde5-b0b9-441f-c6a3-12df645fb9bf"},"source":["\"\"\"\n","======================================================\n","Create raw text dataset from Reuters\n","======================================================\n","This script uses the code from the scikit-learn example \n","plot_out_of_core_classification.py for retrieving the Reuters dataset.\n","The dataset used in this example is Reuters-21578 as provided by the UCI ML\n","repository. It will be automatically downloaded and uncompressed on first run.\n","\"\"\"\n","\n","# Authors: Eustache Diemert <eustache@diemert.fr>\n","\n","# License: BSD 3 clause\n","\n","from __future__ import print_function\n","\n","from glob import glob\n","import itertools\n","import os.path\n","import re\n","import tarfile\n","\n","import numpy as np\n","\n","from sklearn.externals.six.moves import html_parser\n","from sklearn.externals.six.moves import urllib\n","from sklearn.datasets import get_data_home\n","\n","import pickle\n","\n","\n","###############################################################################\n","# Reuters Dataset related routines\n","###############################################################################\n","\n","def _not_in_sphinx():\n","    # Hack to detect whether we are running by the sphinx builder\n","    return '__file__' in globals()\n","\n","class ReutersParser(html_parser.HTMLParser):\n","    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n","\n","    def __init__(self, encoding='latin-1'):\n","        html_parser.HTMLParser.__init__(self)\n","        self._reset()\n","        self.encoding = encoding\n","\n","    def handle_starttag(self, tag, attrs):\n","        method = 'start_' + tag\n","        getattr(self, method, lambda x: None)(attrs)\n","\n","    def handle_endtag(self, tag):\n","        method = 'end_' + tag\n","        getattr(self, method, lambda: None)()\n","\n","    def _reset(self):\n","        self.in_title = 0\n","        self.in_body = 0\n","        self.in_topics = 0\n","        self.in_topic_d = 0\n","        self.title = \"\"\n","        self.body = \"\"\n","        self.topics = []\n","        self.topic_d = \"\"\n","\n","    def parse(self, fd):\n","        self.docs = []\n","        for chunk in fd:\n","            self.feed(chunk.decode(self.encoding))\n","            for doc in self.docs:\n","                yield doc\n","            self.docs = []\n","        self.close()\n","\n","    def handle_data(self, data):\n","        if self.in_body:\n","            self.body += data\n","        elif self.in_title:\n","            self.title += data\n","        elif self.in_topic_d:\n","            self.topic_d += data\n","\n","    def start_reuters(self, attributes):\n","        pass\n","\n","    def end_reuters(self):\n","        self.body = re.sub(r'\\s+', r' ', self.body)\n","        self.docs.append({'title': self.title,\n","                          'body': self.body,\n","                          'topics': self.topics})\n","        self._reset()\n","\n","    def start_title(self, attributes):\n","        self.in_title = 1\n","\n","    def end_title(self):\n","        self.in_title = 0\n","\n","    def start_body(self, attributes):\n","        self.in_body = 1\n","\n","    def end_body(self):\n","        self.in_body = 0\n","\n","    def start_topics(self, attributes):\n","        self.in_topics = 1\n","\n","    def end_topics(self):\n","        self.in_topics = 0\n","\n","    def start_d(self, attributes):\n","        self.in_topic_d = 1\n","\n","    def end_d(self):\n","        self.in_topic_d = 0\n","        self.topics.append(self.topic_d)\n","        self.topic_d = \"\"\n","\n","\n","def stream_reuters_documents(data_path=None):\n","    \"\"\"Iterate over documents of the Reuters dataset.\n","    The Reuters archive will automatically be downloaded and uncompressed if\n","    the `data_path` directory does not exist.\n","    Documents are represented as dictionaries with 'body' (str),\n","    'title' (str), 'topics' (list(str)) keys.\n","    \"\"\"\n","\n","    DOWNLOAD_URL = (\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\")\n","    ARCHIVE_FILENAME = \"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\"\n","\n","    if data_path is None:\n","        data_path = os.path.join(get_data_home(), \"reuters\")\n","    if not os.path.exists(data_path):\n","        \"\"\"Download the dataset.\"\"\"\n","        print(\"downloading dataset (once and for all) into %s\" %\n","              data_path)\n","        os.mkdir(data_path)\n","\n","        def progress(blocknum, bs, size):\n","            total_sz_mb = '%.2f MB' % (size / 1e6)\n","            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n","            if _not_in_sphinx():\n","                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n","                      end='')\n","\n","        archive_path = os.path.join(data_path, data)\n","        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n","                                   reporthook=progress)\n","        if _not_in_sphinx():\n","            print('\\r', end='')\n","        print(\"untarring Reuters dataset...\")\n","        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n","        print(\"done.\")\n","\n","    parser = ReutersParser()\n","    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n","        for doc in parser.parse(open(filename, 'rb')):\n","            yield doc\n","\n","def get_minibatch(doc_iter, size, pos_class):\n","    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n","    Note: size is before excluding invalid docs with no topics assigned.\n","    \"\"\"\n","    data = [(u'{title}\\n\\n{body}'.format(**doc), doc['topics'])\n","            for doc in itertools.islice(doc_iter, size)\n","            if doc['topics']]\n","\n","    # If there's no data, just return empty lists.    \n","    if not len(data):\n","        return np.asarray([], dtype=int), np.asarray([], dtype=int).tolist()\n","    \n","    # Otherwise, retrieve the articles and class labels. zip just splits apart\n","    # the two variables.\n","    X_text, y = zip(*data)\n","\n","    # Convert X_text and y from tuples to lists.    \n","    X_text = list(X_text)    \n","    y = list(y)\n","    \n","    # Convert the class labels to a list.\n","    #y = np.asarray(y, dtype=int).tolist()    \n","    \n","    # For some reason, some of these articles are just whitespace. Look for \n","    # these and remove them. \n","    toRemove = []\n","    docNum = 0\n","    \n","    # For each article...\n","    for article in X_text:\n","        # If the article is just whitespace, or is empty, we'll remove it        \n","        if article.isspace() or (article == \"\"):\n","            toRemove.append(docNum)\n","            \n","        docNum += 1\n","    \n","    # Remove the empty articles. Do this in reverse order so as not to corrupt\n","    # the indeces as we go.\n","    toRemove.reverse()\n","    for i in toRemove:\n","        del X_text[i]\n","        del y[i]\n","    \n","    return X_text, y\n","\n","\n","\n","def iter_minibatches(doc_iter, minibatch_size):\n","    \"\"\"Generator of minibatches.\"\"\"\n","    X_text, y = get_minibatch(doc_iter, minibatch_size)\n","    while len(X_text):\n","        yield X_text, y\n","        X_text, y = get_minibatch(doc_iter, minibatch_size)\n","\n","###############################################################################\n","# Main\n","###############################################################################\n","\n","# Iterator over parsed Reuters SGML files.\n","data_stream = stream_reuters_documents()\n","\n","# The Reuter's dataset includes many different classes, but we're just going to\n","# do binary classification. We'll use 'acq' (articles related to \n","# \"acquisitions\"--one of the most prevalent classes in the dataset) as the \n","# positive class, and all other article topics will be used as negative \n","# examples.\n","positive_class = 'acq'\n","\n","# Retrieve a set of examples from the dataset to use as the training set, then \n","# another set of examples to use as the test set. The actual number will\n","# be smaller because it will exclude \"invalid docs with no topics assigned\".\n","X_train_raw, y_train_raw = get_minibatch(data_stream, 8000, positive_class)\n","X_test_raw, y_test_raw = get_minibatch(data_stream, 2000, positive_class)\n","\n","print(\"Train set is %d documents\" % (len(y_train_raw)))\n","print(\"Test set is %d documents\" % (len(y_test_raw)))\n","\n","# Dump the dataset to a pickle file.\n","pickle.dump((X_train_raw, y_train_raw, X_test_raw, y_test_raw), open(\"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\", \"wb\"))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["downloading dataset (once and for all) into /root/scikit_learn_data/reuters\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e6fa389fa1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# another set of examples to use as the test set. The actual number will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;31m# be smaller because it will exclude \"invalid docs with no topics assigned\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m \u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0mX_test_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-e6fa389fa1b8>\u001b[0m in \u001b[0;36mget_minibatch\u001b[0;34m(doc_iter, size, pos_class)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m    164\u001b[0m     data = [(u'{title}\\n\\n{body}'.format(**doc), doc['topics'])\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             if doc['topics']]\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-e6fa389fa1b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mNote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mexcluding\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mno\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m     data = [(u'{title}\\n\\n{body}'.format(**doc), doc['topics'])\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             if doc['topics']]\n","\u001b[0;32m<ipython-input-3-e6fa389fa1b8>\u001b[0m in \u001b[0;36mstream_reuters_documents\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    144\u001b[0m                       end='')\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0marchive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n\u001b[1;32m    148\u001b[0m                                    reporthook=progress)\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"DX9WOtv7fWwY","executionInfo":{"status":"aborted","timestamp":1623171182651,"user_tz":-270,"elapsed":56,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}}},"source":["#!/usr/bin/env python\n","\"\"\"\n","Run k-NN classification on the Reuters text dataset using LSA.\n","This script leverages modules in scikit-learn for performing tf-idf and SVD.\n","Classification is performed using k-NN with k=5 (majority wins).\n","The script measures the accuracy of plain tf-idf as a baseline, then LSA to\n","show the improvement.\n","@author: Chris McCormick\n","\"\"\"\n","\n","import pickle\n","import time\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import Normalizer\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","\n","###############################################################################\n","#  Load the raw text dataset.\n","###############################################################################\n","\n","print(\"Loading dataset...\")\n","\n","# The raw text dataset is stored as tuple in the form:\n","# (X_train_raw, y_train_raw, X_test_raw, y_test)\n","# The 'filtered' dataset excludes any articles that we failed to retrieve\n","# fingerprints for.\n","raw_text_dataset = pickle.load( open( \"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\", \"rb\" ) )\n","X_train_raw = raw_text_dataset[0]\n","y_train_labels = raw_text_dataset[1] \n","X_test_raw = raw_text_dataset[2]\n","y_test_labels = raw_text_dataset[3]\n","\n","# The Reuters dataset consists of ~100 categories. However, we are going to\n","# simplify this to a binary classification problem. The 'positive class' will\n","# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n","# other articles will be negative.\n","y_train = [\"acq\" in y for y in y_train_labels]\n","y_test = [\"acq\" in y for y in y_test_labels]\n","\n","print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n","print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))\n","\n","\n","###############################################################################\n","#  Use LSA to vectorize the articles.\n","###############################################################################\n","\n","# Tfidf vectorizer:\n","#   - Strips out “stop words”\n","#   - Filters out terms that occur in more than half of the docs (max_df=0.5)\n","#   - Filters out terms that occur in only one document (min_df=2).\n","#   - Selects the 10,000 most frequently occuring words in the corpus.\n","#   - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n","#     document length on the tf-idf values. \n","vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n","                             min_df=2, stop_words='english',\n","                             use_idf=True)\n","\n","# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n","# (\"transform\").\n","X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n","\n","print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])\n","\n","print(\"\\nPerforming dimensionality reduction using LSA\")\n","t0 = time.time()\n","\n","# Project the tfidf vectors onto the first N principal components.\n","# Though this is significantly fewer features than the original tfidf vector,\n","# they are stronger features, and the accuracy is higher.\n","svd = TruncatedSVD(100)\n","lsa = make_pipeline(svd, Normalizer(copy=False))\n","\n","# Run SVD on the training data, then project the training data.\n","X_train_lsa = lsa.fit_transform(X_train_tfidf)\n","\n","print(\"  done in %.3fsec\" % (time.time() - t0))\n","\n","explained_variance = svd.explained_variance_ratio_.sum()\n","print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n","\n","\n","# Now apply the transformations to the test data as well.\n","X_test_tfidf = vectorizer.transform(X_test_raw)\n","X_test_lsa = lsa.transform(X_test_tfidf)\n","\n","\n","###############################################################################\n","#  Run classification of the test articles\n","###############################################################################\n","\n","print(\"\\nClassifying tfidf vectors...\")\n","\n","# Time this step.\n","t0 = time.time()\n","\n","# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n","# and brute-force calculation of distances.\n","knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n","knn_tfidf.fit(X_train_tfidf, y_train)\n","\n","# Classify the test vectors.\n","p = knn_tfidf.predict(X_test_tfidf)\n","\n","# Measure accuracy\n","numRight = 0;\n","for i in range(0,len(p)):\n","    if p[i] == y_test[i]:\n","        numRight += 1\n","\n","print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n","\n","# Calculate the elapsed time (in seconds)\n","elapsed = (time.time() - t0)\n","print(\"  done in %.3fsec\" % elapsed)\n","\n","\n","print(\"\\nClassifying LSA vectors...\")\n","\n","# Time this step.\n","t0 = time.time()\n","\n","# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n","# and brute-force calculation of distances.\n","knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n","knn_lsa.fit(X_train_lsa, y_train)\n","\n","# Classify the test vectors.\n","p = knn_lsa.predict(X_test_lsa)\n","\n","# Measure accuracy\n","numRight = 0;\n","for i in range(0,len(p)):\n","    if p[i] == y_test[i]:\n","        numRight += 1\n","\n","print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n","\n","# Calculate the elapsed time (in seconds)\n","elapsed = (time.time() - t0)    \n","print(\"    done in %.3fsec\" % elapsed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"osv0Cmcmfcsw","executionInfo":{"status":"aborted","timestamp":1623171182675,"user_tz":-270,"elapsed":66,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}}},"source":["#!/usr/bin/env python\n","\"\"\"\n","Perform some analysis on the top components of SVD.\n","This script takes articles from the Reuters classification dataset, then\n","applies LSA to them to create compact feature vectors.\n","We look at some properties of these vectors and the SVD matrix in order to gain\n","some insight into how they work.\n","@author: Chris McCormick\n","\"\"\"\n","\n","import pickle\n","import time\n","import numpy\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import Normalizer\n","\n","from pylab import *\n","\n","import random\n","\n","###############################################################################\n","#  Load the raw text dataset.\n","###############################################################################\n","\n","print(\"Loading dataset...\")\n","\n","# The raw text dataset is stored as tuple in the form:\n","# (X_train_raw, y_train_raw, X_test_raw, y_test)\n","# The 'filtered' dataset excludes any articles that we failed to retrieve\n","# fingerprints for.\n","raw_text_dataset = pickle.load( open( \"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\", \"rb\" ) )\n","X_train_raw = raw_text_dataset[0]\n","\n","print(\"  %d training examples\" % (len(X_train_raw)))\n","\n","###############################################################################\n","#  Use LSA to vectorize the articles.\n","###############################################################################\n","\n","# Tfidf vectorizer:\n","#   - Strips out “stop words”\n","#   - Filters out terms that occur in more than half of the docs (max_df=0.5)\n","#   - Filters out terms that occur in only one document (min_df=2).\n","#   - Selects the 10,000 most frequently occuring words in the corpus.\n","#   - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n","#     document length on the tf-idf values. \n","vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n","                             min_df=2, stop_words='english',\n","                             use_idf=True)\n","\n","# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n","# (\"transform\").\n","X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n","\n","print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])\n","\n","# Get the words that correspond to each of the features.\n","feat_names = vectorizer.get_feature_names()\n","\n","# Print ten random terms from the vocabulary\n","print(\"Some random words in the vocabulary:\")\n","for i in range(0, 10):\n","    featNum = random.randint(0, len(feat_names))\n","    print(\"  %s\" % feat_names[featNum])\n","    \n","print(\"\\nPerforming dimensionality reduction using LSA\")\n","t0 = time.time()\n","\n","# Project the tfidf vectors onto the first N principal components.\n","# Though this is significantly fewer features than the original tfidf vector,\n","# they are stronger features, and the accuracy is higher.\n","svd = TruncatedSVD(100)\n","lsa = make_pipeline(svd, Normalizer(copy=False))\n","\n","# Run SVD on the training data, then project the training data.\n","X_train_lsa = lsa.fit_transform(X_train_tfidf)\n","\n","# The SVD matrix will have one row per component, and one column per feature\n","# of the original data.\n","\n","#for compNum in range(0, 100, 10):\n","for compNum in range(0, 10):\n","\n","    comp = svd.components_[compNum]\n","    \n","    # Sort the weights in the first component, and get the indeces\n","    indeces = numpy.argsort(comp).tolist()\n","    \n","    # Reverse the indeces, so we have the largest weights first.\n","    indeces.reverse()\n","    \n","    # Grab the top 10 terms which have the highest weight in this component.        \n","    terms = [feat_names[weightIndex] for weightIndex in indeces[0:10]]    \n","    weights = [comp[weightIndex] for weightIndex in indeces[0:10]]    \n","   \n","    # Display these terms and their weights as a horizontal bar graph.    \n","    # The horizontal bar graph displays the first item on the bottom; reverse\n","    # the order of the terms so the biggest one is on top.\n","    terms.reverse()\n","    weights.reverse()\n","    positions = arange(10) + .5    # the bar centers on the y axis\n","    \n","    figure(compNum)\n","    barh(positions, weights, align='center')\n","    yticks(positions, terms)\n","    xlabel('Weight')\n","    title('Strongest terms for component %d' % (compNum))\n","    grid(True)\n","    show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WzeUJc9fND-","executionInfo":{"status":"aborted","timestamp":1623171182677,"user_tz":-270,"elapsed":68,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}}},"source":["lines.append(line[:-1])\n","class:\n","\t\t\tlines.append(line)\n","\t\t\t\tyield {\n","\t\t\t\t\t\t'id': int(lines[0]),\n","\t\t\t\t\t\t'title': lines[1],\n","\t\t\t\t\t\t'text': lines[2],\n","\t\t\t\t\t\t'date': lines[3],\n","\t\t\t\t\t\t'time': lines[4],\n","\t\t\t\t\t\t'category': lines[5],\n","\t\t\t\t\t\t'category2': lines[6],\n","\t\t\t\t\t}\n","\t\t\t\t\tlines = []\n","\n","\tdef texts(self):\n","\t\tfor doc in self.docs():\n","\t\t\tyield doc['text']\n","    \"\"\"\n","    interfaces [Persica Corpus](https://sourceforge.net/projects/persica/)\n","    >>> persica = PersicaReader('corpora/persica.csv')\n","    >>> next(persica.docs())['id']\n","    843656\n","    \"\"\"\n","\n","    def __init__(self, csv_file):\n","        self._csv_file = csv_file\n","\n","    def docs(self):\n","        lines = []\n","        for line in codecs.open(self._csv_file, encoding='utf-8-sig'):\n","            line = line.strip()\n","            if line:\n","                if line.endswith(','):\n","                    lines.append(line[:-1])\n","                else:\n","                    lines.append(line)\n","                    yield {\n","                        'id': int(lines[0]),\n","                        'title': lines[1],\n","                        'text': lines[2],\n","                        'date': lines[3],\n","                        'time': lines[4],\n","                        'category': lines[5],\n","                        'category2': lines[6],\n","                    }\n","                    lines = []\n","\n","    def texts(self):\n","        for doc in self.docs():\n","            yield doc['text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrgGNi3QtP_O","executionInfo":{"status":"aborted","timestamp":1623171182679,"user_tz":-270,"elapsed":69,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"}}},"source":["import pandas as pd\n","#!pip install hazm\n","import hazm\n","import matplotlib.pyplot as plt\n","#from stopwords import all_stopwords\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import classification_report\n","\n","\n","normalizer = hazm.Normalizer(persian_numbers=True)\n","tokenizer = hazm.WordTokenizer(replace_numbers=True, replace_hashtags=True)\n","lemmatizer = hazm.Lemmatizer()\n","tagger = hazm.POSTagger(model= \"/content/gdrive/My Drive/Colab Notebooks/NLP/HW03/persica.csv\")\n","\n","def clean_text(sentence):\n","    sentence = normalizer.normalize(sentence)\n","    sentence = tokenizer.tokenize(sentence)\n","    sentence = tagger.tag(sentence)\n","    sentence = [lemmatizer.lemmatize(x[0], pos=x[1]) for x in sentence]\n","    return \" \".join(sentence)\n","\n","data = pd.read_csv('Corpus/persica_org_english_cat.csv')\n","data = data[['title', 'text', 'category2']].dropna()\n","data['cleaned_text'] = data[\"title\"] + \" \" + data['text']\n","\n","doc = \"\"\n","for item in data['cleaned_text']:\n","    doc += item + \" \"\n","doc = doc.split()\n","\n","print(\"Number of Entries: {:,}\\nNumber of tokens: {:,}\\nVocabulary size: {:,}\\nNumber of Classes: {:,}\\n\".format(\n","    data.shape[0], \n","    len(doc),\n","    len(set(doc)),\n","    len(set(data['category2']))\n","))\n","\n","le = LabelEncoder()\n","data['num_cat'] = le.fit_transform(data['category2'])\n","\n","# Shuffling the Data to get a more realistic evaluation\n","data = data.sample(frac=1)\n","\n","# Defining Features and Labels\n","X = data['cleaned_text']\n","y = data['num_cat']\n","\n","# Spliting the Data into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8)\n","\n","# The PipeLine Containing the Vectorizer and The Model With Text PreProcessing\n","sgd = Pipeline([\n","    ('tfidf', TfidfVectorizer(ngram_range=(2, 3), lowercase=False, max_df=0.1, preprocessor=clean_text,\n","                              stop_words=all_stopwords)),\n","    ('clf', SGDClassifier(loss='modified_huber', max_iter=1300, n_jobs=-1, n_iter_no_change=80)),\n","])\n","\n","# Fitting the Model with Training Data\n","sgd.fit(X_train, y_train)\n","\n","# Making Predictions Using the Model on Test Data\n","pred = sgd.predict(X_test)\n","\n","print(classification_report(y_test, pred, target_names=list(le.classes_)))"],"execution_count":null,"outputs":[]}]}